{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVlpZV7Pwc-F",
        "outputId": "573c5212-bc5b-40e7-91d7-cc79f59c0159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.11/dist-packages (0.9)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.99.1)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.16)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.7)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.41.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.12)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.5.0)\n",
            "Requirement already satisfied: llama-index-core<0.14,>=0.13.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.13.1)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.5.0)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.9.1)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.6,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.5.2)\n",
            "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.5.0)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.5.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (3.12.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.2.0)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.3.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (3.5)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (4.3.8)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (80.9.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (0.10.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud==0.1.35 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: pypdf<6,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (5.9.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.1->llama-index) (1.11.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.1->llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.54 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.1->llama-index) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.1->llama-index) (3.26.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.1->llama-index) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "pip install python-docx docx2txt langchain openai llama-index chromadb pydantic gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import io\n",
        "import zipfile\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "import docx\n",
        "from difflib import SequenceMatcher\n",
        "import openai\n",
        "import tempfile  # For handling temporary files\n",
        "\n",
        "# Read OpenAI API key securely from environment variable\n",
        "openai.api_key = os.getenv(\n",
        "    \"OPENAI_API_KEY\",\n",
        "    \"sk-proj-xyFuWhZozv9_3ekFudfbQ28I9QfTJ7I11zFHkIYCsVnI1-OnmodQ6Gd9V9ellTEKzPdGL8tkT3BlbkFJjxFT9bq-4qAeFivRuihmnbLlSziuLTeOYlsmEQrg1VrZVqlUEKthD86PM1qaCbGaj2Eg0q8A\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "oL7wrLSsFZlc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_section_for_keyword(doc: docx.Document, keyword: str, match_text: str, window: int = 500) -> str:\n",
        "    \"\"\"\n",
        "    Find the paragraph where match_text or keyword appears,\n",
        "    then return the closest preceding section heading.\n",
        "    Uses enhanced heading detection logic.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_section_headings(doc: docx.Document) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract paragraphs that look like section headings using regex patterns and heading styles.\n",
        "        \"\"\"\n",
        "        section_headings = []\n",
        "        heading_patterns = [\n",
        "            r\"^(Clause|Section|Article)\\s*\\d+(\\.\\d+)*\",  # Clause 3, Section 4.1, Article 2.3.4\n",
        "            r\"^\\d+(\\.\\d+)+\",                            # 2.1 or 3.1.4 etc.\n",
        "            r\"^\\d+\\)\",                                  # 2) or 3) numbered lists\n",
        "            r\"^[A-Z][A-Z\\s]{3,}$\",                      # ALL CAPS headings (at least 4 letters)\n",
        "            r\"^[IVXLCDM]+\\.\",                           # Roman numerals (e.g., I., II., III.)\n",
        "            r\"^[A-Z]\\.\",                                # Single capital letter followed by period (e.g., A., B.)\n",
        "            r\"^\\([a-z]\\)\",                              # (a), (b) style sub-headings\n",
        "            r\"^\\([ivx]+\\)\",                             # (i), (ii), (iii) style sub-headings\n",
        "        ]\n",
        "        for idx, para in enumerate(doc.paragraphs):\n",
        "            text = para.text.strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            is_heading_style = False\n",
        "            if para.style and 'Heading' in para.style.name:\n",
        "                is_heading_style = True\n",
        "\n",
        "            is_pattern_match = any(re.match(pat, text, re.I) for pat in heading_patterns)\n",
        "\n",
        "            if is_heading_style or is_pattern_match:\n",
        "                section_headings.append({\"index\": idx, \"text\": text})\n",
        "\n",
        "        return section_headings\n",
        "\n",
        "    search_string = match_text if match_text else keyword\n",
        "    if not search_string:\n",
        "        return \"Unknown Section (No Search String Provided)\"\n",
        "\n",
        "    section_headings = extract_section_headings(doc)\n",
        "\n",
        "    # Find paragraph index containing the keyword or match_text\n",
        "    match_index = -1\n",
        "    search_lower = search_string.lower()\n",
        "    for i, para in enumerate(doc.paragraphs):\n",
        "        if search_lower in para.text.lower():\n",
        "            match_index = i\n",
        "            break\n",
        "\n",
        "    if match_index == -1:\n",
        "        return \"Unknown Section (Match Not Found)\"\n",
        "\n",
        "    # Search backward from matched paragraph to find closest preceding heading\n",
        "    for idx in range(match_index, -1, -1):\n",
        "        heading = next((sh for sh in section_headings if sh[\"index\"] == idx), None)\n",
        "        if heading:\n",
        "            return heading[\"text\"]\n",
        "\n",
        "    # If no preceding heading found, optionally return first heading or unknown\n",
        "    if section_headings:\n",
        "        return section_headings[0][\"text\"]\n",
        "\n",
        "    return \"Unknown Section (No Clear Heading Found Before Match)\"\n"
      ],
      "metadata": {
        "id": "h6Bu8vf8MgHD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_comments_in_docx(file_obj, issues: List[Dict]) -> io.BytesIO:\n",
        "    \"\"\"\n",
        "    Insert comments into a DOCX document based on issues found,\n",
        "    returning a BytesIO object with the updated document content.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Load Document from File Object ---\n",
        "    file_obj.seek(0)\n",
        "    doc = docx.Document(file_obj)\n",
        "\n",
        "    # --- 2. Locate Paragraphs for Each Issue ---\n",
        "    issues_with_para_index = []\n",
        "    for issue in issues:\n",
        "        # Use context or issue keyword for searching in paragraphs\n",
        "        search_text = issue.get(\"context\", issue.get(\"issue\", \"\"))\n",
        "        if not search_text or \"Processing Error\" in search_text:\n",
        "            continue  # Skip if no valid search text or error flag present\n",
        "\n",
        "        found = False\n",
        "        # Escape special characters for regex and limit search text length\n",
        "        limited_search_text = search_text[:500]\n",
        "\n",
        "        for i, para in enumerate(doc.paragraphs):\n",
        "            # Check if paragraph contains the limited search text (case-insensitive)\n",
        "            if limited_search_text.lower() in para.text.lower():\n",
        "                # Avoid commenting on a paragraph that's only a section title\n",
        "                section_title = issue.get(\"section\", \"\")\n",
        "                if section_title and section_title.lower().strip() == para.text.lower().strip():\n",
        "                    # If paragraph is section title, comment on next paragraph if exists\n",
        "                    if i + 1 < len(doc.paragraphs):\n",
        "                        issues_with_para_index.append((i + 1, issue))\n",
        "                        found = True\n",
        "                        break\n",
        "                    else:\n",
        "                        # Last paragraph fallback\n",
        "                        issues_with_para_index.append((i, issue))\n",
        "                        found = True\n",
        "                        break\n",
        "                else:\n",
        "                    # Add comment to paragraph with context/keyword\n",
        "                    issues_with_para_index.append((i, issue))\n",
        "                    found = True\n",
        "                    break  # Only one comment per issue\n",
        "\n",
        "        if not found:\n",
        "            print(f\"Warning: Could not find suitable paragraph for comment for issue: {issue.get('issue', 'N/A')}\")\n",
        "\n",
        "    # --- 3. Sort Issues by Paragraph Index Descending ---\n",
        "    # To avoid messing paragraph indices while inserting comments\n",
        "    issues_with_para_index.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # --- 4. Insert Comments into Document ---\n",
        "    for para_index, issue in issues_with_para_index:\n",
        "        if para_index < len(doc.paragraphs):\n",
        "            para = doc.paragraphs[para_index]\n",
        "            issue_type = issue.get(\"issue\", \"N/A\")\n",
        "            context = issue.get(\"details\", \"N/A\")\n",
        "            section = issue.get(\"section\", \"Unknown Section\")\n",
        "\n",
        "            # Format comment text (truncate long context)\n",
        "            comment_text = f\"Issue: {issue_type}\\nSection: {section}\\nContext: {context[:200]}{'...' if len(context) > 200 else ''}\"\n",
        "\n",
        "            # Avoid duplicate comments for the same paragraph\n",
        "            if comment_text not in para.text:\n",
        "                try:\n",
        "                    comment_marker = f\"[Review Needed: {comment_text}]\"\n",
        "                    if comment_marker not in para.text:\n",
        "                        run = para.add_run(f\"  {comment_marker}\")\n",
        "                        run.italic = True\n",
        "                        run.font.color.rgb = docx.shared.RGBColor(0xFF, 0x00, 0x00)  # Red color\n",
        "                except Exception as e:\n",
        "                    print(f\"Error adding comment to paragraph {para_index}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: Paragraph index {para_index} out of bounds for commenting.\")\n",
        "\n",
        "    # --- 5. Save Modified Document to BytesIO ---\n",
        "    bio = io.BytesIO()\n",
        "    try:\n",
        "        doc.save(bio)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving document after adding comments: {e}\")\n",
        "        # Attempt fallback to original file with error note\n",
        "        file_obj.seek(0)\n",
        "        original_content = file_obj.read()\n",
        "        bio = io.BytesIO(original_content)\n",
        "        try:\n",
        "            error_doc = docx.Document(io.BytesIO(original_content))\n",
        "            error_doc.add_paragraph(f\"[ERROR: Failed to add comments due to: {e}. See JSON report for details.]\")\n",
        "            bio = io.BytesIO()\n",
        "            error_doc.save(bio)\n",
        "        except Exception as e_save_error_doc:\n",
        "            print(f\"Further error saving error doc: {e_save_error_doc}\")\n",
        "            file_obj.seek(0)\n",
        "            bio = io.BytesIO(file_obj.read())\n",
        "\n",
        "    bio.seek(0)\n",
        "    return bio\n"
      ],
      "metadata": {
        "id": "ObrUoYwqMjjp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Mandatory Documents List ---\n",
        "MANDATORY_DOCS = {\n",
        "    \"Company Incorporation\": [\n",
        "        \"Articles of Association\",\n",
        "        \"Memorandum of Association\",\n",
        "        \"Board Resolution Template\",\n",
        "        \"Shareholder Resolution Template\",\n",
        "        \"Register of Members and Directors\",\n",
        "        \"Incorporation Application Form\",\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "Ksm7_f0fOHpw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def analyze_documents(uploaded_files):\n",
        "    # Gradio outputs: checklist_out, per_file_out, structured_out, download_zip, download_json\n",
        "    # Need to return placeholders if there's an error before generating outputs\n",
        "    checklist_md = \"\"\n",
        "    per_file_md = \"\"\n",
        "    structured_output = {}\n",
        "    zip_file_path = None # Will store path to temporary zip file\n",
        "    json_file_path = None # Will store path to temporary json file\n",
        "\n",
        "\n",
        "    try:\n",
        "        if not uploaded_files:\n",
        "            return \"⚠️ Please upload one or more .docx files to start analysis.\", \"\", {}, None, None\n",
        "\n",
        "        # uploaded_files is a list of file paths (strings) from Gradio\n",
        "        file_objs = []\n",
        "        for f in uploaded_files:\n",
        "            try:\n",
        "                with open(f, \"rb\") as file_data:\n",
        "                    bio = io.BytesIO(file_data.read())\n",
        "                bio.name = os.path.basename(f)\n",
        "                file_objs.append(bio)\n",
        "            except Exception as e:\n",
        "                 print(f\"Error reading uploaded file {f}: {e}\")\n",
        "                 # Skip this file but continue with others\n",
        "                 continue\n",
        "\n",
        "        if not file_objs:\n",
        "             return \"⚠️ No valid files were read for analysis.\", \"\", {}, None, None\n",
        "\n",
        "        # Identify doc types\n",
        "        docs_text = parse_docx_documents(file_objs)\n",
        "        detected_docs = identify_document_types_with_score(docs_text)\n",
        "        doc_types_list = [d[\"doc_type\"] for d in detected_docs]\n",
        "        uploaded_file_names = [d['file_name'] for d in detected_docs] # Get actual names of processed files\n",
        "\n",
        "        # Determine legal process & checklist\n",
        "        legal_process = \"Company Incorporation\" if any(d in MANDATORY_DOCS[\"Company Incorporation\"] for d in doc_types_list) else \"Unknown\"\n",
        "        mandatory_list = MANDATORY_DOCS.get(legal_process, MANDATORY_DOCS[\"Company Incorporation\"])\n",
        "        checklist_found, checklist_missing = check_mandatory_documents(doc_types_list, mandatory_list)\n",
        "        process_match_score = round(len(checklist_found) / len(mandatory_list), 2) if mandatory_list else 0\n",
        "\n",
        "        checklist_md = (\n",
        "            f\"**Process detected:** {legal_process} — (match hits: {len(checklist_found)}/{len(mandatory_list)})\\n\\n\"\n",
        "            f\"**Checklist found:** {checklist_found}\\n\\n\"\n",
        "            f\"**Checklist missing:** {checklist_missing}\\n\"\n",
        "        )\n",
        "\n",
        "        # Red flag detection\n",
        "        issues_per_file, reviewed_docs = [], []\n",
        "        # Create a dictionary mapping original file names to their BytesIO objects for easy access\n",
        "        file_obj_dict = {f.name: f for f in file_objs}\n",
        "\n",
        "        for d in detected_docs:\n",
        "            fname = d[\"file_name\"]\n",
        "            current_file_obj = file_obj_dict.get(fname)\n",
        "\n",
        "            if current_file_obj is None:\n",
        "                 print(f\"Error: File object for {fname} not found in memory.\")\n",
        "                 issues_per_file.append({\n",
        "                     \"file_name\": fname,\n",
        "                     \"doc_type\": d[\"doc_type\"],\n",
        "                     \"doc_score\": d[\"doc_score\"],\n",
        "                     \"red_flags\": [{\"keyword\": \"Processing Error\", \"context\": \"Internal error: File object not found for analysis.\", \"adgm_present\": False, \"section\": \"N/A\"}],\n",
        "                 })\n",
        "                 continue\n",
        "\n",
        "\n",
        "            try:\n",
        "                current_file_obj.seek(0) # Ensure file pointer is at the beginning\n",
        "                doc_obj = docx.Document(current_file_obj)\n",
        "                red_flags = detect_red_flags_detailed(fname, doc_obj)\n",
        "                issues_per_file.append({\n",
        "                    \"file_name\": fname,\n",
        "                    \"doc_type\": d[\"doc_type\"],\n",
        "                    \"doc_score\": d[\"doc_score\"],\n",
        "                    \"red_flags\": red_flags,\n",
        "                })\n",
        "                # Reset file object pointer before inserting comments\n",
        "                current_file_obj.seek(0)\n",
        "                # Pass the issue structure expected by insert_comments_in_docx\n",
        "                comments_issues = [{\n",
        "                    # Use 'keyword' for 'issue' type in the comment\n",
        "                    \"issue\": rf.get(\"keyword\", \"N/A\"),\n",
        "                    # Use 'context' for 'details' in the comment\n",
        "                    \"details\": rf.get(\"context\", \"No context\"),\n",
        "                    \"section\": rf.get(\"section\", \"Unknown Section\")\n",
        "                } for rf in red_flags]\n",
        "\n",
        "                marked_bio = insert_comments_in_docx(current_file_obj, comments_issues)\n",
        "                marked_bio.name = f\"reviewed_{fname}\" # Explicitly set name for Gradio\n",
        "                reviewed_docs.append(marked_bio) # Append BytesIO object directly\n",
        "            except Exception as e:\n",
        "                print(f\"Error during red flag detection or commenting for {fname}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc() # Print traceback for debugging\n",
        "\n",
        "                issues_per_file.append({\n",
        "                     \"file_name\": fname,\n",
        "                     \"doc_type\": d[\"doc_type\"],\n",
        "                     \"doc_score\": d[\"doc_score\"],\n",
        "                     \"red_flags\": [{\"keyword\": \"Processing Error\", \"context\": f\"Error during analysis: {e}\", \"adgm_present\": False, \"section\": \"N/A\"}],\n",
        "                 })\n",
        "                # Still try to include the original file in the zip if possible\n",
        "                if current_file_obj:\n",
        "                    current_file_obj.seek(0)\n",
        "                    error_bio = io.BytesIO(current_file_obj.read())\n",
        "                    error_bio.name = f\"error_{fname}\"\n",
        "                    reviewed_docs.append(error_bio)\n",
        "\n",
        "\n",
        "        # Create ZIP of reviewed docs and save to a temporary file\n",
        "        if reviewed_docs:\n",
        "             with tempfile.NamedTemporaryFile(suffix=\".zip\", delete=False) as tmp_zip:\n",
        "                 zip_buffer = io.BytesIO()\n",
        "                 with zipfile.ZipFile(zip_buffer, \"w\") as zf:\n",
        "                     for bio in reviewed_docs:\n",
        "                         bio.seek(0)\n",
        "                         zf.writestr(bio.name, bio.read())\n",
        "                 tmp_zip.write(zip_buffer.getvalue())\n",
        "                 zip_file_path = tmp_zip.name\n",
        "             print(f\"Generated temporary ZIP file: {zip_file_path}, size: {os.path.getsize(tmp_zip.name)} bytes\")\n",
        "        else:\n",
        "             print(\"No documents to include in ZIP.\")\n",
        "             zip_file_path = None\n",
        "\n",
        "\n",
        "        # Build issues summary (Simplified for the requested output structure)\n",
        "        severity_map = {\n",
        "            \"jurisdiction (non-ADGM)\": \"High\",\n",
        "            \"jurisdiction (ADGM reference missing)\": \"High\",\n",
        "            \"reference to UAE federal courts\": \"High\",\n",
        "            \"explicit non-ADGM governing law\": \"High\",\n",
        "            \"signature section missing\": \"Medium\",\n",
        "            \"signature details potentially missing\": \"Low\",\n",
        "            \"ambiguous wording\": \"Low\",\n",
        "            \"governing law (non-ADGM)\": \"High\",\n",
        "            \"governing law (ADGM law reference missing)\": \"High\",\n",
        "            \"governing law clause unclear or incomplete\": \"Medium\",\n",
        "            \"force majeure clause (ADGM context unclear)\": \"Low\",\n",
        "            \"indemnity/liability clause (ADGM context unclear)\": \"Medium\",\n",
        "            \"confidentiality clause (ADGM context unclear)\": \"Low\",\n",
        "            \"GDPR or non-ADGM data protection reference\": \"Low\",\n",
        "            \"non-AED/USD currency reference (convertibility unclear)\": \"Low\",\n",
        "            \"potentially outdated date reference\": \"Low\",\n",
        "            \"termination clause (details unclear/missing)\": \"Medium\",\n",
        "            \"assignment/transfer restrictions (ADGM context unclear)\": \"Medium\",\n",
        "            \"specific performance/equitable remedies clause (ADGM context unclear)\": \"Medium\",\n",
        "            \"Processing Error\": \"High\" # Mark processing errors clearly\n",
        "        }\n",
        "        suggestion_map = {\n",
        "            \"jurisdiction (non-ADGM)\": \"Ensure jurisdiction is explicitly set to ADGM Courts.\",\n",
        "            \"jurisdiction (ADGM reference missing)\": \"Ensure jurisdiction is explicitly set to ADGM Courts.\",\n",
        "            \"reference to UAE federal courts\": \"Replace references with ADGM Courts.\",\n",
        "            \"explicit non-ADGM governing law\": \"Explicitly state that the agreement is governed by ADGM law.\",\n",
        "            \"signature section missing\": \"Add authorized signatory section with name, title, and date fields.\",\n",
        "            \"signature details potentially missing\": \"Verify name, title, and date fields are present for signatories.\",\n",
        "            \"ambiguous wording ('may')\": \"Replace 'may' with clear, mandatory language (e.g., 'shall') if a requirement is intended.\", # More specific suggestion\n",
        "            \"ambiguous wording ('might')\": \"Replace 'might' with clear, mandatory language (e.g., 'shall') if a requirement is intended.\",\n",
        "            \"ambiguous wording ('should')\": \"Replace 'should' with 'shall' if a mandatory action is intended.\",\n",
        "            \"ambiguous wording ('could')\": \"Replace 'could' with clear language indicating possibility or requirement.\",\n",
        "            \"ambiguous wording ('possibly')\": \"Replace 'possibly' with clear language indicating possibility or requirement.\",\n",
        "            \"ambiguous wording ('endeavour')\": \"Replace 'endeavour' with a firm commitment (e.g., 'shall use best efforts' or 'shall').\",\n",
        "            \"ambiguous wording ('endeavor')\": \"Replace 'endeavor' with a firm commitment (e.g., 'shall use best efforts' or 'shall').\",\n",
        "            \"ambiguous wording ('best efforts')\": \"Define 'best efforts' or replace with a clearer standard.\",\n",
        "            \"ambiguous wording ('reasonable efforts')\": \"Define 'reasonable efforts' or replace with a clearer standard.\",\n",
        "\n",
        "            \"governing law (non-ADGM)\": \"Ensure governing law is explicitly set to ADGM law.\",\n",
        "            \"governing law (ADGM law reference missing)\": \"Ensure governing law is explicitly set to ADGM law.\",\n",
        "            \"governing law clause unclear or incomplete\": \"Clarify and complete the governing law clause, specifying ADGM law.\",\n",
        "            \"force majeure clause (ADGM context unclear)\": \"Review and ensure the Force Majeure clause aligns with ADGM regulations or common practice.\",\n",
        "            \"indemnity/liability clause (ADGM context unclear)\": \"Review and ensure Indemnity and Liability clauses align with ADGM regulations or common practice.\",\n",
        "            \"confidentiality clause (ADGM context unclear)\": \"Review and ensure the Confidentiality clause aligns with ADGM regulations or common practice.\",\n",
        "            \"GDPR or non-ADGM data protection reference\": \"Assess if non-ADGM data protection references are necessary and ensure compliance with ADGM data protection regulations.\",\n",
        "            \"non-AED/USD currency reference (convertibility unclear)\": \"Confirm if non-AED/USD currencies are acceptable or if values should be in AED/USD as per ADGM requirements.\",\n",
        "            \"potentially outdated date reference\": \"Verify the date is correct and relevant to the agreement's validity.\",\n",
        "            \"termination clause (details unclear/missing)\": \"Review and complete the termination clause, including notice periods and grounds.\",\n",
        "            \"assignment/transfer restrictions (ADGM context unclear)\": \"Review assignment/transfer clauses for ADGM compliance.\",\n",
        "            \"specific performance/equitable remedies clause (ADGM context unclear)\": \"Review remedies clause for ADGM compliance regarding specific performance or equitable relief.\",\n",
        "            \"Processing Error\": \"Analysis failed for this document. Please check the file format or contact support.\"\n",
        "        }\n",
        "\n",
        "        issues_summary_list = [] # Flat list for the structured output format\n",
        "        for file_info in issues_per_file:\n",
        "            for rf in file_info[\"red_flags\"]:\n",
        "                keyword = rf.get(\"keyword\", \"N/A\")\n",
        "                severity = severity_map.get(keyword, \"Low\")\n",
        "                # Use the specific keyword match for ambiguous wording to get a better suggestion\n",
        "                if \"ambiguous wording\" in keyword:\n",
        "                     suggestion = suggestion_map.get(keyword, \"Review clause for clarity.\")\n",
        "                else:\n",
        "                     suggestion = suggestion_map.get(keyword, \"Review clause for ADGM compliance.\")\n",
        "\n",
        "\n",
        "                issues_summary_list.append({\n",
        "                    \"document\": file_info.get(\"doc_type\", file_info.get(\"file_name\", \"Unknown File\")),\n",
        "                    \"section\": rf.get(\"section\", \"Unknown Section\"),\n",
        "                    \"issue\": rf.get(\"keyword\", \"Unknown Issue Type\"), # Use keyword as the issue type\n",
        "                    \"severity\": severity,\n",
        "                    \"suggestion\": suggestion\n",
        "                })\n",
        "\n",
        "\n",
        "        structured_output = {\n",
        "            \"process\": legal_process,\n",
        "            \"documents_uploaded\": len(uploaded_file_names),\n",
        "            \"required_documents\": len(mandatory_list),\n",
        "            \"missing_document\": checklist_missing[0] if checklist_missing else None, # Report only the first missing or None\n",
        "            \"issues_found\": issues_summary_list\n",
        "        }\n",
        "\n",
        "\n",
        "        # Create JSON report and save to a temporary file\n",
        "        try:\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False) as tmp_json:\n",
        "                json_bytes = json.dumps(structured_output, indent=2).encode(\"utf-8\")\n",
        "                tmp_json.write(json_bytes)\n",
        "                json_file_path = tmp_json.name\n",
        "            print(f\"Generated temporary JSON file: {json_file_path}, size: {os.path.getsize(tmp_json.name)} bytes\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating temporary JSON report file: {e}\")\n",
        "            json_file_path = None # Return None on error\n",
        "\n",
        "\n",
        "        # Generate per_file_md (Keep detailed markdown for per-file view)\n",
        "        per_file_md = \"**Per-file summary:**\\n\\n\"\n",
        "        if issues_per_file:\n",
        "            for file_info in issues_per_file:\n",
        "                per_file_md += f\"  **{file_info['file_name']}** — type: {file_info['doc_type']} (score {file_info['doc_score']})\\n\"\n",
        "                if file_info[\"red_flags\"]:\n",
        "                    for rf in file_info[\"red_flags\"]:\n",
        "                        keyword = rf.get('keyword', 'N/A')\n",
        "                        section = rf.get('section', 'Unknown Section')\n",
        "                        context = rf.get('context', 'No context')\n",
        "                        adgm_present_info = f\" (ADGM context: {rf.get('adgm_present', 'N/A')})\" if 'adgm_present' in rf else ''\n",
        "                        per_file_md += f\"    - **Issue:** {keyword} {adgm_present_info}\\n\"\n",
        "                        per_file_md += f\"      Section: {section}\\n\"\n",
        "                        per_file_md += f\"      Context: {context[:150]}{'...' if len(context)>150 else ''}\\n\" # Limit context length\n",
        "                        per_file_md += f\"      Severity: {severity_map.get(keyword, 'Low')}\\n\"\n",
        "                        # Use the specific keyword match for ambiguous wording to get a better suggestion in markdown too\n",
        "                        if \"ambiguous wording\" in keyword:\n",
        "                            suggestion = suggestion_map.get(keyword, \"Review clause for clarity.\")\n",
        "                        else:\n",
        "                            suggestion = suggestion_map.get(keyword, \"Review clause.\")\n",
        "                        per_file_md += f\"      Suggestion: {suggestion}\\n\"\n",
        "                    per_file_md += \"\\n\" # Add newline after red flags for a file\n",
        "                else:\n",
        "                    per_file_md += \"    No red flags found.\\n\\n\" # Add newline even if no flags\n",
        "        else:\n",
        "            per_file_md += \"No files were successfully analyzed.\"\n",
        "\n",
        "        # Return file paths instead of BytesIO objects\n",
        "        return checklist_md, per_file_md, structured_output, zip_file_path, json_file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected top-level error occurred during analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print traceback for debugging\n",
        "        # Return error messages and None for files if a top-level exception occurs\n",
        "        return f\"An unexpected error occurred during analysis: {e}\", \"\", {}, None, None\n",
        "\n"
      ],
      "metadata": {
        "id": "L2VMBTIVOgKu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Gradio UI ---\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ADGM-Compliant Corporate Agent - Document Review\")\n",
        "    gr.Markdown(\"Upload legal document(s) (.docx) for analysis and receive compliance feedback.\")\n",
        "\n",
        "    uploaded_files = gr.File(\n",
        "        file_types=[\".docx\"],\n",
        "        file_count=\"multiple\",\n",
        "        type=\"filepath\",  # returns list of file paths\n",
        "        label=\"Upload .docx files\"\n",
        "    )\n",
        "\n",
        "    analyze_btn = gr.Button(\"Analyze Documents\")\n",
        "\n",
        "    checklist_out = gr.Markdown()\n",
        "    per_file_out = gr.Markdown()\n",
        "    structured_out = gr.JSON()\n",
        "    # Change outputs to accept file paths\n",
        "    download_zip = gr.File(label=\"Download Reviewed Documents (ZIP)\", interactive=False)\n",
        "    download_json = gr.File(label=\"Download Analysis Report (JSON)\", interactive=False)\n",
        "\n",
        "    analyze_btn.click(\n",
        "        analyze_documents,\n",
        "        inputs=[uploaded_files],\n",
        "        outputs=[checklist_out, per_file_out, structured_out, download_zip, download_json]\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "48rNMY4OPciU",
        "outputId": "a75f6c8b-8391-43ff-aeed-daffc43ce779"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://891a81b1df2bec9243.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://891a81b1df2bec9243.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Avvl8a-EP4oQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}